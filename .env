# --- Model Settings ---

# Provide name for the GGUF model to be used
# Same name will be used for naming the model downloaded from hugging face
MODEL_NAME=Meta-Llama-3.2-1B-Instruct.gguf

# --- Docker Compose File Settings---
COMPOSE_FILE_PATH=./docker-compose.yml

# --- Open Web UI Settings ---
OPENWEBUI_PORT=3000
OPENAI_API_LLAMA_CPP_BASE=http://localhost:11434/v1

# --- LLAMA CPP Settings ---
GPU_LAYERS=30
N_CTX=2048
N_BATCH=512

# Enables or disables chat title generation - default False
ENABLE_TITLE_GENERATION=False

# Enables or disables chat tag generation - default False
ENABLE_TAGS_GENERATION=False

# Hugging Face Model Downloader Configuration
# Update HF_REPO & HF_MODEL_FILE if user wants to try out some other HF gguf models
HF_TOKEN=<your-token>
HF_REPO="bartowski/Llama-3.2-1B-Instruct-GGUF"
HF_MODEL_FILE="Llama-3.2-1B-Instruct-Q4_K_M.gguf"

# Model Save Path
DEST_DIR="llama-cpp-service/models"
